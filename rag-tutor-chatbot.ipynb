{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "462f08ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a559b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fb3e9525",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "212f2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f13d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8c923221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(docs_dir: Path):\n",
    "    \"\"\"Load .txt and .pdf from docs_dir -> LangChain Documents list.\"\"\"\n",
    "    docs = []\n",
    "    for p in docs_dir.glob(\"**/*\"):\n",
    "        if p.is_file() and p.stat().st_size > 0:  # Check if file is not empty\n",
    "            if p.suffix.lower() == \".txt\":\n",
    "                docs += TextLoader(str(p), encoding=\"utf-8\").load()\n",
    "            elif p.suffix.lower() == \".pdf\":\n",
    "                docs += PyPDFLoader(str(p)).load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65035a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tiktoken\n",
    "    def count_tokens(text, model_name=\"gemma2-9b-it\"):\n",
    "        try:\n",
    "            enc = tiktoken.encoding_for_model(model_name)\n",
    "            return len(enc.encode(text))\n",
    "        except KeyError:\n",
    "            # Fallback to cl100k_base encoding (used by GPT-3.5/4)\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            return len(enc.encode(text))\n",
    "except Exception:\n",
    "    def count_tokens(text, model_name=None):\n",
    "        # fallback heuristic: ~0.75 * words\n",
    "        return max(1, int(len(text.split()) * 0.75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6777193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def build_splits(docs, max_tokens=750, overlap_tokens=120):\n",
    "    \"\"\"Token-aware chunking with rich metadata for Sinhala text.\"\"\"\n",
    "    splits = []\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        words = text.split()\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = min(len(words), start + max_tokens * 2)  # safe upper bound\n",
    "            while end > start:\n",
    "                candidate = \" \".join(words[start:end])\n",
    "                if count_tokens(candidate) <= max_tokens:\n",
    "                    break\n",
    "                end -= 1\n",
    "            if end == start:\n",
    "                end = min(len(words), start + max_tokens)\n",
    "            chunk_text = \" \".join(words[start:end])\n",
    "            new_meta = dict(doc.metadata or {})\n",
    "            new_meta.update({\n",
    "                \"_chunk_start\": start,\n",
    "                \"_chunk_end\": end,\n",
    "                \"source\": doc.metadata.get(\"source\", getattr(doc, \"source\", \"unknown\")),\n",
    "                \"orig_filename\": doc.metadata.get(\"file_name\") or doc.metadata.get(\"source\", \"unknown\")\n",
    "            })\n",
    "            splits.append(Document(page_content=chunk_text, metadata=new_meta))\n",
    "            start = max(start + 1, end - int(overlap_tokens / 0.75))\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a002113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings_and_persist(splits, embeddings, persist_path: Path):\n",
    "    \"\"\"Batch embeddings and cache them (persistence).\"\"\"\n",
    "    if persist_path.exists():\n",
    "        return FAISS.load_local(str(persist_path), embeddings, allow_dangerous_deserialization=True)\n",
    "    texts = [d.page_content for d in splits]\n",
    "    batch_size = 32\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        vectors.extend(embeddings.embed_documents(batch))\n",
    "    vs = FAISS.from_documents(splits, embeddings)\n",
    "    vs.save_local(str(persist_path))\n",
    "    return vs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aef3d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, docs, top_k=3):\n",
    "    \"\"\"Re-rank documents using cross-encoder for better relevance.\"\"\"\n",
    "    pairs = [[query, d.page_content] for d in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    ranked = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)\n",
    "    return [d for _, d in ranked[:top_k]]\n",
    "\n",
    "def ensure_vectorstore(splits, embeddings: Embeddings, persist_path: Path):\n",
    "    \"\"\"Create or load FAISS index.\"\"\"\n",
    "    return make_embeddings_and_persist(splits, embeddings, persist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b878f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = Path(\"docs\")\n",
    "docs = load_docs(docs_dir)\n",
    "splits = build_splits(docs, max_tokens=750, overlap_tokens=120)  # Token-aware chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d44cfb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_path = Path(\"vectorstore\")\n",
    "vectorstore = make_embeddings_and_persist(splits, embeddings, persist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "84ec973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "725569c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system = SystemMessagePromptTemplate.from_template(\n",
    "    \"ඔබ සිංහලෙන් පිළිතුරු දෙන සහායකයෙක් වනවා. සපයන ලද මූලාශ්‍ර පමණක් භාවිතා කර පිළිතුරු දෙන්න. \"\n",
    "    \"පිළිතුර මූලාශ්‍රවල නොමැති නම්, මම නොදනිමි කියා සහ සොයන්නට හෝ වැඩි විස්තර ඉල්ලන්නට යෝජනා කරන්න. \"\n",
    "    \"පිළිතුරු සංක්ෂිප්තව තබා ගන්න.\"\n",
    ")\n",
    "\n",
    "human = HumanMessagePromptTemplate.from_template(\n",
    "    \"පරිශීලක ප්‍රශ්නය: {question}\\n\\nපහත සන්දර්භය භාවිතා කරන්න: {context}\\n\\nමූලාශ්‍ර රේඛාගතව උපුටා දක්වන්න: [1], [2] වැනි ආකාරයකින්.\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system, human])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e46f2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "class EnhancedQAChain:\n",
    "    \"\"\"Enhanced QA chain with re-ranking and better formatting.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, retriever, cross_encoder):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.cross_encoder = cross_encoder\n",
    "\n",
    "    def retrieve_and_rerank(self, query):\n",
    "        candidates = self.retriever.get_relevant_documents(query)\n",
    "        reranked = rerank_with_cross_encoder(query, candidates, top_k=3)\n",
    "        return reranked\n",
    "\n",
    "    def format_docs(self, docs):\n",
    "        \"\"\"Format documents with citations.\"\"\"\n",
    "        formatted = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            source = doc.metadata.get(\"orig_filename\", \"unknown\")\n",
    "            formatted.append(f\"[{i}] {doc.page_content} (Source: {source})\")\n",
    "        return \"\\n\\n\".join(formatted)\n",
    "\n",
    "    def invoke(self, query):\n",
    "        docs = self.retrieve_and_rerank(query)\n",
    "        context = self.format_docs(docs)\n",
    "\n",
    "        full_prompt = prompt.format_messages(question=query, context=context)\n",
    "\n",
    "        response = self.llm.invoke(full_prompt)\n",
    "\n",
    "        return response.content\n",
    "\n",
    "qa_chain = EnhancedQAChain(llm, retriever, cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d40800eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: AI හි ප්‍රධාන වර්ග අතර ඒවායේ යෙදුම් වලට අනුව \n",
      "1. Narrow AI (Weak AI) \n",
      "2. General AI (Strong AI) \n",
      "3. Super AI \n",
      "\n",
      "ලෙස වේ. [1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = \"AI හි ප්‍රධාන වර්ග\"\n",
    "    result = qa_chain.invoke(query)\n",
    "    print(\"Answer:\", result)\n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d995ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: AI යනු කුමක්ද?\n",
      "Top-3 retrieved documents' sources: [None]\n",
      "Query: කෘතිම බුද්ධිකත්වයේ මූලික කරුණු මොනවාද?\n",
      "Top-3 retrieved documents' sources: [None]\n",
      "Precision at 3: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(qa_pairs, retriever, cross_encoder, k=3):\n",
    "    \"\"\"Measure precision at k for retrieval + re-ranking.\"\"\"\n",
    "    hits = 0\n",
    "    for q, expected_filename in qa_pairs:\n",
    "        candidates = retriever.get_relevant_documents(q)\n",
    "        reranked = rerank_with_cross_encoder(q, candidates, top_k=k)\n",
    "        print(f\"Query: {q}\")\n",
    "        print(f\"Top-{k} retrieved documents' sources: {[d.metadata.get('orig_filename') for d in reranked]}\")\n",
    "        if any(expected_filename in (d.metadata.get(\"orig_filename\") or \"\") for d in reranked):\n",
    "            hits += 1\n",
    "    return hits / len(qa_pairs) if qa_pairs else 0\n",
    "\n",
    "eval_queries = [\n",
    "    (\"AI යනු කුමක්ද?\", \"lesson1.txt\"),\n",
    "    (\"කෘතිම බුද්ධිකත්වයේ මූලික කරුණු මොනවාද?\", \"lesson1.txt\"),\n",
    "]\n",
    "\n",
    "try:\n",
    "    precision = precision_at_k(eval_queries, retriever, cross_encoder, k=3)\n",
    "    print(f\"Precision at {3}: {precision:.2%}\")\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
